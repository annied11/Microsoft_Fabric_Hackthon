{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the dataset to explore its structure and contents\u001b[39;00m\n\u001b[1;32m      2\u001b[0m mat_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstudent/student-mat.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m student_mat_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(mat_data_path, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m por_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstudent/student-por.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m student_por_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(por_data_path, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the dataset to explore its structure and contents\n",
    "mat_data_path = \"student/student-mat.csv\"\n",
    "student_mat_data = pd.read_csv(mat_data_path, delimiter=';')\n",
    "\n",
    "por_data_path = \"student/student-por.csv\"\n",
    "student_por_data = pd.read_csv(por_data_path, delimiter=';')\n",
    "\n",
    "combined_data = pd.concat([student_mat_data, student_por_data], ignore_index=True)\n",
    "\n",
    "combined_data.head(), combined_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on G3(Final Year Grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error:  5.607307011948271\n",
      "R-squared:  -0.13965947376433285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Selecting features and target for the model\n",
    "features = ['sex', 'age', 'famsize', 'Medu', 'Fedu', 'Mjob', 'Fjob']\n",
    "target = 'G3'\n",
    "\n",
    "# Preparing the data\n",
    "X = student_data[features]\n",
    "y = student_data[target]\n",
    "\n",
    "# One-hot encoding categorical variables\n",
    "categorical_features = ['sex', 'famsize', 'Mjob', 'Fjob']\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Setting up the column transformer with OneHotEncoder\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', one_hot_encoder, categorical_features)],\n",
    "    remainder='passthrough')\n",
    "\n",
    "# Creating a pipeline that first transforms data then fits a Random Forest model\n",
    "rf_model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=0))\n",
    "])\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Fitting the Random Forest model\n",
    "rf_model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predicting test set results with the Random Forest model\n",
    "y_rf_pred = rf_model_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluating the Random Forest model\n",
    "rf_mse = mean_squared_error(y_test, y_rf_pred)\n",
    "rf_rmse = np.sqrt(rf_mse)\n",
    "rf_r2 = r2_score(y_test, y_rf_pred)\n",
    "\n",
    "# Output the performance metrics\n",
    "print(\"Root Mean Squared Error: \", rf_rmse)\n",
    "print(\"R-squared: \", rf_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/microsoft_hackthon/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "15 fits failed out of a total of 45.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/microsoft_hackthon/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/microsoft_hackthon/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/microsoft_hackthon/lib/python3.12/site-packages/sklearn/pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"/opt/anaconda3/envs/microsoft_hackthon/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/envs/microsoft_hackthon/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/envs/microsoft_hackthon/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/envs/microsoft_hackthon/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [        nan         nan         nan -0.17508094 -0.15850638 -0.14952317\n",
      " -0.17508094 -0.15850638 -0.14952317]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>-0.149523</td>\n",
       "      <td>{'regressor__max_features': 'sqrt', 'regressor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GradientBoostingRegressor</td>\n",
       "      <td>0.016423</td>\n",
       "      <td>{'regressor__learning_rate': 0.01, 'regressor_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.053240</td>\n",
       "      <td>{'regressor__alpha': 100}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model  best_score  \\\n",
       "0      RandomForestRegressor   -0.149523   \n",
       "1  GradientBoostingRegressor    0.016423   \n",
       "2                      Ridge    0.053240   \n",
       "\n",
       "                                         best_params  \n",
       "0  {'regressor__max_features': 'sqrt', 'regressor...  \n",
       "1  {'regressor__learning_rate': 0.01, 'regressor_...  \n",
       "2                          {'regressor__alpha': 100}  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Define the models and parameters to be tested\n",
    "model_params = {\n",
    "    'RandomForestRegressor': {\n",
    "        'model': RandomForestRegressor(random_state=0),\n",
    "        'params': {\n",
    "            'regressor__n_estimators': [50, 100, 200],\n",
    "            'regressor__max_features': ['auto', 'sqrt', 'log2']\n",
    "        }\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'model': GradientBoostingRegressor(random_state=0),\n",
    "        'params': {\n",
    "            'regressor__n_estimators': [50, 100, 200],\n",
    "            'regressor__learning_rate': [0.01, 0.1, 0.2],\n",
    "            'regressor__max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'Ridge': {\n",
    "        'model': Ridge(),\n",
    "        'params': {\n",
    "            'regressor__alpha': [1, 10, 100, 1000]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Loop through each model/parameters combination and perform Grid Search\n",
    "for model_name, mp in model_params.items():\n",
    "    # Setup the pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', mp['model'])\n",
    "    ])\n",
    "    clf = GridSearchCV(pipe, mp['params'], cv=5, return_train_score=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    results.append({\n",
    "        'model': model_name,\n",
    "        'best_score': clf.best_score_,\n",
    "        'best_params': clf.best_params_\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Gender</th>\n",
       "      <th>EthnicGroup</th>\n",
       "      <th>ParentEduc</th>\n",
       "      <th>LunchType</th>\n",
       "      <th>TestPrep</th>\n",
       "      <th>ParentMaritalStatus</th>\n",
       "      <th>PracticeSport</th>\n",
       "      <th>IsFirstChild</th>\n",
       "      <th>NrSiblings</th>\n",
       "      <th>TransportMeans</th>\n",
       "      <th>WklyStudyHours</th>\n",
       "      <th>MathScore</th>\n",
       "      <th>ReadingScore</th>\n",
       "      <th>WritingScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>married</td>\n",
       "      <td>regularly</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>school_bus</td>\n",
       "      <td>&lt; 5</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>married</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5 - 10</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>single</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>yes</td>\n",
       "      <td>4.0</td>\n",
       "      <td>school_bus</td>\n",
       "      <td>&lt; 5</td>\n",
       "      <td>87</td>\n",
       "      <td>93</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>married</td>\n",
       "      <td>never</td>\n",
       "      <td>no</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5 - 10</td>\n",
       "      <td>45</td>\n",
       "      <td>56</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>married</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>school_bus</td>\n",
       "      <td>5 - 10</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Gender EthnicGroup          ParentEduc     LunchType TestPrep  \\\n",
       "0           0  female         NaN   bachelor's degree      standard     none   \n",
       "1           1  female     group C        some college      standard      NaN   \n",
       "2           2  female     group B     master's degree      standard     none   \n",
       "3           3    male     group A  associate's degree  free/reduced     none   \n",
       "4           4    male     group C        some college      standard     none   \n",
       "\n",
       "  ParentMaritalStatus PracticeSport IsFirstChild  NrSiblings TransportMeans  \\\n",
       "0             married     regularly          yes         3.0     school_bus   \n",
       "1             married     sometimes          yes         0.0            NaN   \n",
       "2              single     sometimes          yes         4.0     school_bus   \n",
       "3             married         never           no         1.0            NaN   \n",
       "4             married     sometimes          yes         0.0     school_bus   \n",
       "\n",
       "  WklyStudyHours  MathScore  ReadingScore  WritingScore  \n",
       "0            < 5         71            71            74  \n",
       "1         5 - 10         69            90            88  \n",
       "2            < 5         87            93            91  \n",
       "3         5 - 10         45            56            42  \n",
       "4         5 - 10         76            78            75  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_data_path = \"student_score.csv\"\n",
    "df = pd.read_csv(score_data_path)\n",
    "\n",
    "# Display the first few rows of the dataset and the column names to understand its structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.1829618018382412e-14\n",
      "R-squared: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Adding an average score column\n",
    "df['AverageScore'] = df[['MathScore', 'ReadingScore', 'WritingScore']].mean(axis=1)\n",
    "\n",
    "# Preparing the data with OneHotEncoder for categorical variables\n",
    "categorical_features = ['Gender', 'EthnicGroup', 'ParentEduc', 'LunchType', 'TestPrep', 'ParentMaritalStatus', 'PracticeSport', 'IsFirstChild', 'WklyStudyHours']\n",
    "numeric_features = ['MathScore', 'ReadingScore', 'WritingScore']  # dropped for a real prediction model, included here for data prep demonstration\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "        ('num', 'passthrough', numeric_features)\n",
    "    ])\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X = df[categorical_features + numeric_features]\n",
    "y = df['AverageScore']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# # For the purpose of saving preprocessor\n",
    "# preprocessor.fit(X_train)\n",
    "\n",
    "# X_train_transformed = preprocessor.transform(X_train)\n",
    "# # Assuming you have X_test defined similarly\n",
    "# X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# # To save\n",
    "# joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "\n",
    "# Creating a pipeline that includes preprocessing and a regression model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Fitting the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting test set results\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m     clf \u001b[38;5;241m=\u001b[39m GridSearchCV(pipe, spec[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m], cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     47\u001b[0m     clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m---> 48\u001b[0m     \u001b[43mresults\u001b[49m\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: name,\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_score\u001b[39m\u001b[38;5;124m'\u001b[39m: clf\u001b[38;5;241m.\u001b[39mbest_score_,\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_params\u001b[39m\u001b[38;5;124m'\u001b[39m: clf\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[1;32m     52\u001b[0m     })\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Convert results to DataFrame for easy viewing\u001b[39;00m\n\u001b[1;32m     55\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# Model configurations for GridSearch, updated to remove 'normalize' and add scaling\n",
    "models = {\n",
    "    'LinearRegression': {\n",
    "        'model': LinearRegression(),\n",
    "        'params': {\n",
    "            'regressor__fit_intercept': [True, False]\n",
    "        }\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'model': RandomForestRegressor(),\n",
    "        'params': {\n",
    "            'regressor__n_estimators': [10, 50, 100],\n",
    "            'regressor__max_features': ['auto', 'sqrt', 'log2']\n",
    "        }\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'model': GradientBoostingRegressor(),\n",
    "        'params': {\n",
    "            'regressor__n_estimators': [100, 200],\n",
    "            'regressor__learning_rate': [0.01, 0.1],\n",
    "            'regressor__max_depth': [3, 5]\n",
    "        }\n",
    "    },\n",
    "    'Ridge': {\n",
    "        'model': Ridge(),\n",
    "        'params': {\n",
    "            'regressor__alpha': [1, 10, 100]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Adjusting the pipeline to include StandardScaler for Linear Regression\n",
    "for name, spec in models.items():\n",
    "    if name == 'LinearRegression':\n",
    "        pipe = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('scaler', StandardScaler()),  # Adding a scaler here\n",
    "            ('regressor', spec['model'])\n",
    "        ])\n",
    "    else:\n",
    "        pipe = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', spec['model'])\n",
    "        ])\n",
    "\n",
    "    clf = GridSearchCV(pipe, spec['params'], cv=5, scoring='neg_mean_squared_error')\n",
    "    clf.fit(X_train, y_train)\n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'best_score': clf.best_score_,\n",
    "        'best_params': clf.best_params_\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved as 'best_predict_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "best_model = max(results, key=lambda x: x['best_score'])\n",
    "\n",
    "# Now let's find the actual model object from the clf that corresponds to the best model\n",
    "if best_model['model'] == 'LinearRegression':\n",
    "    best_estimator = clf.best_estimator_  # clf should be your last GridSearchCV object for LinearRegression\n",
    "else:\n",
    "    # Loop through clf objects to find the matching model \n",
    "    best_params = best_model['best_params']\n",
    "    final_pipe = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('scaler', StandardScaler()) if best_model['model'] == 'LinearRegression' else None,\n",
    "        ('regressor', models[best_model['model']]['model'])\n",
    "    ])\n",
    "    final_clf = GridSearchCV(final_pipe, {f'regressor__{k}': [v] for k, v in best_params.items()}, cv=5, scoring='neg_mean_squared_error')\n",
    "    final_clf.fit(X_train, y_train)\n",
    "    best_estimator = final_clf.best_estimator_\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_estimator, 'best_predict_model.pkl')\n",
    "print(\"Best model saved as 'best_predict_model.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding an average score column\n",
    "df['AverageScore'] = df[['MathScore', 'ReadingScore', 'WritingScore']].mean(axis=1)\n",
    "\n",
    "# Preparing the data with OneHotEncoder for categorical variables\n",
    "categorical_features = ['Gender', 'EthnicGroup', 'ParentEduc', 'LunchType', 'TestPrep', 'ParentMaritalStatus', 'PracticeSport', 'IsFirstChild', 'WklyStudyHours']\n",
    "numeric_features = ['MathScore', 'ReadingScore', 'WritingScore']  # dropped for a real prediction model, included here for data prep demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/microsoft_hackthon/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 464us/step - loss: 1087.4786 - val_loss: 0.4329\n",
      "Epoch 2/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 422us/step - loss: 0.2376 - val_loss: 0.1697\n",
      "Epoch 3/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408us/step - loss: 0.1053 - val_loss: 0.1234\n",
      "Epoch 4/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412us/step - loss: 0.0763 - val_loss: 0.0800\n",
      "Epoch 5/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409us/step - loss: 0.0506 - val_loss: 0.0600\n",
      "Epoch 6/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396us/step - loss: 0.0378 - val_loss: 0.0545\n",
      "Epoch 7/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415us/step - loss: 0.0297 - val_loss: 0.0309\n",
      "Epoch 8/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 426us/step - loss: 0.0215 - val_loss: 0.0239\n",
      "Epoch 9/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409us/step - loss: 0.0179 - val_loss: 0.0207\n",
      "Epoch 10/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 410us/step - loss: 0.0165 - val_loss: 0.0336\n",
      "Epoch 11/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 410us/step - loss: 0.0166 - val_loss: 0.0297\n",
      "Epoch 12/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step - loss: 0.0281 - val_loss: 0.0131\n",
      "Epoch 13/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393us/step - loss: 0.0197 - val_loss: 0.0259\n",
      "Epoch 14/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 422us/step - loss: 0.0199 - val_loss: 0.0183\n",
      "Epoch 15/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386us/step - loss: 0.0197 - val_loss: 0.0241\n",
      "Epoch 16/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.0249 - val_loss: 0.0089\n",
      "Epoch 17/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 0.0141 - val_loss: 0.0554\n",
      "Epoch 18/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - loss: 0.0565 - val_loss: 0.0204\n",
      "Epoch 19/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 0.0399 - val_loss: 0.0077\n",
      "Epoch 20/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - loss: 0.0145 - val_loss: 0.0157\n",
      "Epoch 21/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step - loss: 0.0269 - val_loss: 0.0059\n",
      "Epoch 22/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448us/step - loss: 0.0103 - val_loss: 0.0114\n",
      "Epoch 23/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 413us/step - loss: 0.0161 - val_loss: 0.0693\n",
      "Epoch 24/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 435us/step - loss: 0.0262 - val_loss: 0.0108\n",
      "Epoch 25/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 492us/step - loss: 0.0229 - val_loss: 0.0698\n",
      "Epoch 26/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424us/step - loss: 0.0143 - val_loss: 0.1059\n",
      "Epoch 27/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 417us/step - loss: 0.0719 - val_loss: 0.0053\n",
      "Epoch 28/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 432us/step - loss: 0.0066 - val_loss: 0.0227\n",
      "Epoch 29/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398us/step - loss: 0.0272 - val_loss: 0.0086\n",
      "Epoch 30/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 417us/step - loss: 0.0083 - val_loss: 0.0057\n",
      "Epoch 31/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419us/step - loss: 0.0110 - val_loss: 0.0519\n",
      "Epoch 32/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 433us/step - loss: 0.0406 - val_loss: 0.0022\n",
      "Epoch 33/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399us/step - loss: 0.0027 - val_loss: 0.0030\n",
      "Epoch 34/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416us/step - loss: 0.0356 - val_loss: 0.0070\n",
      "Epoch 35/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 444us/step - loss: 0.0184 - val_loss: 0.0219\n",
      "Epoch 36/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418us/step - loss: 0.0113 - val_loss: 0.0098\n",
      "Epoch 37/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418us/step - loss: 0.0055 - val_loss: 0.0026\n",
      "Epoch 38/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469us/step - loss: 0.0206 - val_loss: 0.0033\n",
      "Epoch 39/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421us/step - loss: 0.0140 - val_loss: 0.0022\n",
      "Epoch 40/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392us/step - loss: 0.0071 - val_loss: 0.0048\n",
      "Epoch 41/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389us/step - loss: 0.0183 - val_loss: 0.0050\n",
      "Epoch 42/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452us/step - loss: 0.0347 - val_loss: 0.0028\n",
      "Epoch 43/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421us/step - loss: 0.0162 - val_loss: 0.0068\n",
      "Epoch 44/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407us/step - loss: 0.0100 - val_loss: 0.0032\n",
      "Epoch 45/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420us/step - loss: 0.0012 - val_loss: 0.0083\n",
      "Epoch 46/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405us/step - loss: 0.0188 - val_loss: 0.0818\n",
      "Epoch 47/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400us/step - loss: 0.0373 - val_loss: 0.0017\n",
      "Epoch 48/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398us/step - loss: 0.0040 - val_loss: 0.0190\n",
      "Epoch 49/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419us/step - loss: 0.0047 - val_loss: 0.0024\n",
      "Epoch 50/50\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469us/step - loss: 0.0093 - val_loss: 0.0021\n",
      "Test MSE: 0.0010049943812191486\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m5,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,469</span> (181.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m46,469\u001b[0m (181.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,489</span> (60.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,489\u001b[0m (60.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,980</span> (121.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m30,980\u001b[0m (121.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define the preprocessing for numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "        ('num', StandardScaler(), numeric_features)  # Scaling numeric features\n",
    "    ])\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X = df[categorical_features + numeric_features]\n",
    "y = df['AverageScore']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# For the purpose of saving preprocessor\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "# Assuming you have X_test defined similarly\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# To save\n",
    "joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "\n",
    "# Fit the preprocessing pipeline on the training data and transform training and testing data\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Neural network architecture\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Adjusted to match the input feature size\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test MSE: {test_loss}')\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('average_score_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
